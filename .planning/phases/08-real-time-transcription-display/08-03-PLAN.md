---
phase: 08-real-time-transcription-display
plan: 03
type: execute
wave: 2
depends_on: ["08-01", "08-02"]
files_modified:
  - src-tauri/src/overlay.rs
  - src-tauri/src/actions.rs
  - src/overlay/TranscriptionDisplay.tsx
  - src-tauri/src/lib.rs
  - src-tauri/tauri.conf.json
autonomous: true

must_haves:
  truths:
    - "Transcription display window appears above the recording overlay when recording starts"
    - "Partial transcription text streams into the display during recording"
    - "When recording stops, streaming text is replaced by final batch transcription"
    - "Transcription display window hides after final text is shown and pasted"
    - "Streaming and batch transcription coexist without conflict"
  artifacts:
    - path: "src-tauri/src/overlay.rs"
      provides: "create_transcription_display, show/hide functions for transcription window"
      contains: "transcription_display"
    - path: "src-tauri/src/actions.rs"
      provides: "Streaming integration in setup_segment_listener and TranscribeAction start/stop"
      contains: "StreamingSession"
  key_links:
    - from: "src-tauri/src/actions.rs"
      to: "src-tauri/src/overlay.rs"
      via: "show_transcription_display called in TranscribeAction::start"
      pattern: "show_transcription_display"
    - from: "src-tauri/src/actions.rs"
      to: "src-tauri/src/commands/streaming.rs"
      via: "StreamingSession used to manage streaming lifecycle"
      pattern: "StreamingSession"
    - from: "src-tauri/src/actions.rs"
      to: "transcription_display window"
      via: "window.emit for partial/final events"
      pattern: "emit.*transcription-partial"
    - from: "src-tauri/src/overlay.rs"
      to: "transcription-display.html"
      via: "WebviewWindowBuilder points to transcription display entry"
      pattern: "transcription-display"
---

<objective>
Wire the backend streaming pipeline (Plan 01) to the frontend display component (Plan 02). Create the transcription display window in Rust, integrate streaming into the recording action lifecycle, and connect the frontend to receive window-targeted events.

Purpose: This is the integration plan that makes everything work end-to-end. Without it, the backend and frontend are disconnected pieces.

Output: Complete real-time transcription display that shows during dictation and gets replaced by final batch result.
</objective>

<execution_context>
@C:\Users\hsbaz\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\hsbaz\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-real-time-transcription-display/08-RESEARCH.md
@.planning/phases/08-real-time-transcription-display/08-01-SUMMARY.md
@.planning/phases/08-real-time-transcription-display/08-02-SUMMARY.md

Key existing code to reference:
@src-tauri/src/overlay.rs - Recording overlay creation, positioning, show/hide pattern
@src-tauri/src/actions.rs - TranscribeAction start/stop, STREAMING_STATE, setup_segment_listener
@src-tauri/src/commands/streaming.rs - StreamingSession, StreamingTranscriptionEvent
@src/overlay/TranscriptionDisplay.tsx - Frontend display component
@src-tauri/src/lib.rs - Window creation in initialize_core_logic, invoke_handler
@src-tauri/tauri.conf.json - Window security config
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create transcription display window management in Rust</name>
  <files>src-tauri/src/overlay.rs, src-tauri/src/lib.rs</files>
  <action>
Add transcription display window creation and management to `src-tauri/src/overlay.rs`.

**Constants (add near existing overlay constants):**
```rust
const TRANSCRIPTION_DISPLAY_WIDTH: f64 = 600.0;
const TRANSCRIPTION_DISPLAY_HEIGHT: f64 = 220.0;  // 200px content + padding
const DISPLAY_SPACING: f64 = 12.0;  // Gap between transcription display and recording overlay
```

**Add these functions:**

1. `create_transcription_display(app_handle: &AppHandle)` - Creates the window:
```rust
pub fn create_transcription_display(app_handle: &AppHandle) {
    // Check if window already exists
    if app_handle.get_webview_window("transcription_display").is_some() {
        return;
    }

    // Calculate position: centered horizontally, positioned above where recording overlay would be
    let (x, y) = calculate_transcription_display_position(app_handle)
        .unwrap_or((200.0, 200.0));

    match WebviewWindowBuilder::new(
        app_handle,
        "transcription_display",
        tauri::WebviewUrl::App("src/overlay/transcription-display.html".into()),
    )
    .title("Transcription")
    .position(x, y)
    .resizable(false)
    .inner_size(TRANSCRIPTION_DISPLAY_WIDTH, TRANSCRIPTION_DISPLAY_HEIGHT)
    .shadow(false)
    .maximizable(false)
    .minimizable(false)
    .closable(false)
    .accept_first_mouse(true)
    .decorations(false)
    .always_on_top(true)
    .skip_taskbar(true)
    .transparent(true)
    .focused(false)
    .visible(false)
    .build()
    {
        Ok(_) => debug!("Transcription display window created (hidden)"),
        Err(e) => debug!("Failed to create transcription display window: {}", e),
    }
}
```

2. `calculate_transcription_display_position(app_handle: &AppHandle) -> Option<(f64, f64)>` - Position above recording overlay:
```rust
fn calculate_transcription_display_position(app_handle: &AppHandle) -> Option<(f64, f64)> {
    let settings = settings::get_settings(app_handle);

    // If overlay position is None, don't show transcription display either
    if settings.overlay_position == OverlayPosition::None {
        return None;
    }

    let monitor = get_monitor_with_cursor(app_handle)?;
    let work_area = monitor.work_area();
    let scale = monitor.scale_factor();
    let work_area_width = work_area.size.width as f64 / scale;
    let work_area_y = work_area.position.y as f64 / scale;
    let work_area_x = work_area.position.x as f64 / scale;
    let work_area_height = work_area.size.height as f64 / scale;

    // Center horizontally
    let x = work_area_x + (work_area_width - TRANSCRIPTION_DISPLAY_WIDTH) / 2.0;

    // Position based on overlay position setting
    let y = match settings.overlay_position {
        OverlayPosition::Top => {
            // Below recording overlay (overlay is at top)
            work_area_y + OVERLAY_TOP_OFFSET + OVERLAY_HEIGHT + DISPLAY_SPACING
        },
        OverlayPosition::Bottom | OverlayPosition::None => {
            // Above recording overlay (overlay is at bottom)
            work_area_y + work_area_height - OVERLAY_BOTTOM_OFFSET - OVERLAY_HEIGHT - DISPLAY_SPACING - TRANSCRIPTION_DISPLAY_HEIGHT
        },
        OverlayPosition::FollowCursor => {
            // For follow cursor, position above center of screen (static while recording)
            // Don't follow cursor for the display - it would be distracting
            work_area_y + work_area_height - OVERLAY_BOTTOM_OFFSET - OVERLAY_HEIGHT - DISPLAY_SPACING - TRANSCRIPTION_DISPLAY_HEIGHT
        },
    };

    Some((x, y))
}
```

3. `show_transcription_display(app_handle: &AppHandle)`:
```rust
pub fn show_transcription_display(app_handle: &AppHandle) {
    let settings = settings::get_settings(app_handle);
    if settings.overlay_position == OverlayPosition::None {
        return;
    }

    // Ensure window exists
    if app_handle.get_webview_window("transcription_display").is_none() {
        create_transcription_display(app_handle);
    }

    // Update position
    if let Some(window) = app_handle.get_webview_window("transcription_display") {
        if let Some((x, y)) = calculate_transcription_display_position(app_handle) {
            let _ = window.set_position(tauri::Position::Logical(tauri::LogicalPosition { x, y }));
        }
        let _ = window.show();
        let _ = window.emit("show-transcription-display", ());
    }
}
```

4. `hide_transcription_display(app_handle: &AppHandle)`:
```rust
pub fn hide_transcription_display(app_handle: &AppHandle) {
    if let Some(window) = app_handle.get_webview_window("transcription_display") {
        let _ = window.emit("hide-transcription-display", ());
        // Delay hide to allow fade-out animation
        let window_clone = window.clone();
        std::thread::spawn(move || {
            std::thread::sleep(std::time::Duration::from_millis(300));
            let _ = window_clone.hide();
        });
    }
}
```

**In `src-tauri/src/lib.rs`:**
Add `utils::create_transcription_display(app_handle);` right after `utils::create_recording_overlay(app_handle);` in `initialize_core_logic`. Note: check if `utils` re-exports overlay functions or if they're called directly from `overlay`. Follow the existing pattern - the recording overlay is called via `utils::create_recording_overlay` so the transcription display should follow the same routing. If utils.rs wraps overlay.rs functions, add the wrapper there too.
  </action>
  <verify>
Run `cd C:\Users\hsbaz\leadrscribe && cargo check --manifest-path src-tauri/Cargo.toml 2>&1 | head -20` to verify compilation.
  </verify>
  <done>Transcription display window creation, positioning (above recording overlay), show/hide functions exist in overlay.rs. Window is created at app startup. cargo check passes.</done>
</task>

<task type="auto">
  <name>Task 2: Integrate streaming into TranscribeAction start/stop</name>
  <files>src-tauri/src/actions.rs</files>
  <action>
Modify `src-tauri/src/actions.rs` to show/hide the transcription display and manage the streaming session lifecycle during recording.

**Part A: Add imports (near top of file)**

Add these imports:
```rust
use crate::commands::streaming::{StreamingSession, StreamingTranscriptionEvent};
use crate::managers::streaming_buffer::StreamingBuffer;
```

**Part B: Modify TranscribeAction::start()**

After `STREAMING_STATE.start_recording();` and after `show_recording_overlay(app);`, add:
```rust
// Show transcription display for real-time text
crate::overlay::show_transcription_display(app);

// Initialize streaming session for real-time transcription
let streaming_session = app.state::<Arc<StreamingSession>>();
{
    let mut buffer = streaming_session.buffer.lock().unwrap();
    buffer.reset();
    *streaming_session.is_active.lock().unwrap() = true;
    streaming_session.partial_texts.lock().unwrap().clear();
    streaming_session.pending_chunks.store(0, std::sync::atomic::Ordering::Release);
}
```

**Part C: Modify TranscribeAction::stop()**

After the existing batch transcription completes, before pasting:
```rust
// Send final result to transcription display, replacing streaming partials
if let Some(window) = ah.get_webview_window("transcription_display") {
    let _ = window.emit("transcription-final", serde_json::json!({
        "text": transcription.clone(),
    }));
}

// Stop streaming session
if let Ok(session) = ah.try_state::<Arc<StreamingSession>>() {
    *session.is_active.lock().unwrap() = false;
}
```

And add `hide_transcription_display` calls everywhere `hide_recording_overlay` is called in the stop() method. There are multiple exit paths (success, error, empty transcription). Search for all instances of `hide_recording_overlay` in stop() and add a corresponding:
```rust
crate::overlay::hide_transcription_display(&ah_clone);
```
  </action>
  <verify>
Run `cd C:\Users\hsbaz\leadrscribe && cargo check --manifest-path src-tauri/Cargo.toml 2>&1 | head -20` to verify compilation.
  </verify>
  <done>TranscribeAction::start shows transcription display and initializes streaming session. TranscribeAction::stop sends final result and hides transcription display in all exit paths. cargo check passes.</done>
</task>

<task type="auto">
  <name>Task 3: Add streaming buffer feeding in setup_segment_listener and wire frontend events</name>
  <files>src-tauri/src/actions.rs, src/overlay/TranscriptionDisplay.tsx</files>
  <action>
**Part A: Modify the existing setup_segment_listener() in actions.rs**

The function `setup_segment_listener()` exists at line ~97 in actions.rs. It uses `std::sync::Once` (LISTENER_INITIALIZED) and sets up a single `app.listen("audio-segment", ...)` callback. Inside that existing callback, AFTER the existing segment transcription logic (after the `tm.transcribe_segment_async(...)` call block), add the streaming buffer feeding logic:

```rust
// Feed into streaming buffer for real-time display
let streaming_session = app_clone.state::<Arc<StreamingSession>>();
if *streaming_session.is_active.lock().unwrap() {
    let chunk_opt = {
        let mut buffer = streaming_session.buffer.lock().unwrap();
        buffer.add_segment(segment_event.samples.clone())
    };

    if let Some((chunk_samples, chunk_index)) = chunk_opt {
        let pending = streaming_session.pending_chunks.load(std::sync::atomic::Ordering::Acquire);
        if pending < 2 {
            streaming_session.pending_chunks.fetch_add(1, std::sync::atomic::Ordering::AcqRel);
            let tm = tm_clone.clone();
            let session = streaming_session.inner().clone();
            let app_for_emit = app_clone.clone();

            std::thread::spawn(move || {
                match tm.transcribe(chunk_samples) {
                    Ok(text) if !text.is_empty() => {
                        // Store partial text
                        {
                            let mut partials = session.partial_texts.lock().unwrap();
                            if partials.len() <= chunk_index {
                                partials.resize(chunk_index + 1, String::new());
                            }
                            partials[chunk_index] = text.clone();
                        }

                        // Emit to transcription display window via window-targeted event
                        if let Some(window) = app_for_emit.get_webview_window("transcription_display") {
                            let _ = window.emit("transcription-partial", serde_json::json!({
                                "text": text,
                                "chunk_index": chunk_index,
                            }));
                        }
                    }
                    Ok(_) => { /* empty transcription, skip */ }
                    Err(e) => {
                        log::error!("Streaming chunk {} transcription failed: {}", chunk_index, e);
                    }
                }
                session.pending_chunks.fetch_sub(1, std::sync::atomic::Ordering::AcqRel);
            });
        } else {
            log::debug!("Dropping streaming chunk {} - concurrency limit reached", chunk_index);
        }
    }
}
```

IMPORTANT: This goes INSIDE the existing `app.listen("audio-segment", move |event| { ... })` callback in setup_segment_listener(), after the existing `if let Ok(segment_event)` block's segment processing logic. Do NOT create a second listener. The Once guard means only one listener is ever created.

IMPORTANT concurrency limiting: Use `pending_chunks` AtomicUsize. Before spawning a transcription thread, check `pending_chunks.load() < 2`. If at limit, skip the chunk (drop it) rather than queue it - this prevents GPU memory buildup. Log when chunks are dropped.

Note on approach: We use window-targeted events (`window.emit()`) rather than Tauri Channels. Channels require frontend-initiated command invocations, but our streaming is initiated from Rust (shortcut handler). Window-targeted events are acceptable for this moderate throughput (one event every 2-3 seconds). This is NOT the "event spam" the research warned about -- that was about emitting every word.

**Part B: Verify TranscriptionDisplay.tsx event handling matches backend**

The component from Plan 02 should already handle these events. Verify and update if needed to ensure the event payload parsing matches what the backend emits:

For `transcription-partial`:
```typescript
const unlistenPartial = await listen<{ text: string; chunk_index: number }>(
  "transcription-partial",
  (event) => {
    const { text } = event.payload;
    const newWords = text.split(/\s+/).filter(w => w.length > 0);
    setWords(prev => [...prev, ...newWords]);
  }
);
```

For `transcription-final`:
```typescript
const unlistenFinal = await listen<{ text: string }>(
  "transcription-final",
  (event) => {
    const { text } = event.payload;
    const finalWords = text.split(/\s+/).filter(w => w.length > 0);
    setWords(finalWords);
    setIsListening(false);
  }
);
```

For `show-transcription-display`:
```typescript
const unlistenShow = await listen("show-transcription-display", () => {
  setIsVisible(true);
  setIsListening(true);
  setWords([]);
});
```

For `hide-transcription-display`:
```typescript
const unlistenHide = await listen("hide-transcription-display", () => {
  setIsVisible(false);
  setIsListening(false);
});
```

Ensure all listeners are cleaned up on unmount.
  </action>
  <verify>
Run `cd C:\Users\hsbaz\leadrscribe && cargo check --manifest-path src-tauri/Cargo.toml 2>&1 | head -20` and `cd C:\Users\hsbaz\leadrscribe && bun run build 2>&1 | tail -10` to verify both backend and frontend compile.
  </verify>
  <done>
- Audio segments feed into StreamingBuffer inside the existing setup_segment_listener callback
- Chunks transcribed asynchronously with concurrency limit of 2
- Partial results emitted to transcription display window via window.emit
- Frontend component receives and renders partial/final events correctly
- Both cargo check and bun run build pass
  </done>
</task>

</tasks>

<verification>
1. `cargo check --manifest-path src-tauri/Cargo.toml` passes
2. `bun run build` passes
3. overlay.rs has create/show/hide functions for transcription_display window
4. actions.rs shows transcription display in TranscribeAction::start()
5. actions.rs hides transcription display in ALL stop() exit paths
6. Streaming buffer fed from within the existing setup_segment_listener callback (no second listener)
7. Concurrency limited to 2 pending chunks
8. Events sent via window.emit to "transcription_display" window (not Channels)
9. TranscriptionDisplay.tsx handles transcription-partial and transcription-final events
</verification>

<success_criteria>
- During recording, transcription display window appears above recording overlay
- Partial text streams in every 2-3 seconds as chunks are transcribed
- Words animate in with Framer Motion spring animation
- When recording stops, final batch transcription replaces streaming text
- Display hides after text is pasted
- No GPU OOM from concurrent transcriptions (limited to 2)
- Existing batch transcription flow still works correctly
</success_criteria>

<output>
After completion, create `.planning/phases/08-real-time-transcription-display/08-03-SUMMARY.md`
</output>
