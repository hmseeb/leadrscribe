---
phase: 08-real-time-transcription-display
plan: 03
type: execute
wave: 2
depends_on: ["08-01", "08-02"]
files_modified:
  - src-tauri/src/overlay.rs
  - src-tauri/src/actions.rs
  - src/overlay/TranscriptionDisplay.tsx
  - src-tauri/src/lib.rs
  - src-tauri/tauri.conf.json
autonomous: true

must_haves:
  truths:
    - "Transcription display window appears above the recording overlay when recording starts"
    - "Partial transcription text streams into the display during recording"
    - "When recording stops, streaming text is replaced by final batch transcription"
    - "Transcription display window hides after final text is shown and pasted"
    - "Streaming and batch transcription coexist without conflict"
  artifacts:
    - path: "src-tauri/src/overlay.rs"
      provides: "create_transcription_display, show/hide functions for transcription window"
      contains: "transcription_display"
    - path: "src-tauri/src/actions.rs"
      provides: "Streaming integration in TranscribeAction start/stop"
      contains: "start_streaming_transcription"
  key_links:
    - from: "src-tauri/src/actions.rs"
      to: "src-tauri/src/overlay.rs"
      via: "show_transcription_display called in TranscribeAction::start"
      pattern: "show_transcription_display"
    - from: "src-tauri/src/actions.rs"
      to: "src-tauri/src/commands/streaming.rs"
      via: "StreamingSession used to manage streaming lifecycle"
      pattern: "StreamingSession"
    - from: "src/overlay/TranscriptionDisplay.tsx"
      to: "@tauri-apps/api/core"
      via: "Channel for receiving streaming transcription events"
      pattern: "Channel"
    - from: "src-tauri/src/overlay.rs"
      to: "transcription-display.html"
      via: "WebviewWindowBuilder points to transcription display entry"
      pattern: "transcription-display"
---

<objective>
Wire the backend streaming pipeline (Plan 01) to the frontend display component (Plan 02). Create the transcription display window in Rust, integrate streaming into the recording action lifecycle, and connect the frontend to receive Channel-based streaming events.

Purpose: This is the integration plan that makes everything work end-to-end. Without it, the backend and frontend are disconnected pieces.

Output: Complete real-time transcription display that shows during dictation and gets replaced by final batch result.
</objective>

<execution_context>
@C:\Users\hsbaz\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\hsbaz\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-real-time-transcription-display/08-RESEARCH.md
@.planning/phases/08-real-time-transcription-display/08-01-SUMMARY.md
@.planning/phases/08-real-time-transcription-display/08-02-SUMMARY.md

Key existing code to reference:
@src-tauri/src/overlay.rs - Recording overlay creation, positioning, show/hide pattern
@src-tauri/src/actions.rs - TranscribeAction start/stop, STREAMING_STATE
@src-tauri/src/commands/streaming.rs - StreamingSession, StreamingTranscriptionEvent, Channel commands
@src/overlay/TranscriptionDisplay.tsx - Frontend display component
@src-tauri/src/lib.rs - Window creation in initialize_core_logic, invoke_handler
@src-tauri/tauri.conf.json - Window security config
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create transcription display window management in Rust</name>
  <files>src-tauri/src/overlay.rs, src-tauri/src/lib.rs</files>
  <action>
Add transcription display window creation and management to `src-tauri/src/overlay.rs`.

**Constants (add near existing overlay constants):**
```rust
const TRANSCRIPTION_DISPLAY_WIDTH: f64 = 600.0;
const TRANSCRIPTION_DISPLAY_HEIGHT: f64 = 220.0;  // 200px content + padding
const DISPLAY_SPACING: f64 = 12.0;  // Gap between transcription display and recording overlay
```

**Add these functions:**

1. `create_transcription_display(app_handle: &AppHandle)` - Creates the window:
```rust
pub fn create_transcription_display(app_handle: &AppHandle) {
    // Check if window already exists
    if app_handle.get_webview_window("transcription_display").is_some() {
        return;
    }

    // Calculate position: centered horizontally, positioned above where recording overlay would be
    let (x, y) = calculate_transcription_display_position(app_handle)
        .unwrap_or((200.0, 200.0));

    match WebviewWindowBuilder::new(
        app_handle,
        "transcription_display",
        tauri::WebviewUrl::App("src/overlay/transcription-display.html".into()),
    )
    .title("Transcription")
    .position(x, y)
    .resizable(false)
    .inner_size(TRANSCRIPTION_DISPLAY_WIDTH, TRANSCRIPTION_DISPLAY_HEIGHT)
    .shadow(false)
    .maximizable(false)
    .minimizable(false)
    .closable(false)
    .accept_first_mouse(true)
    .decorations(false)
    .always_on_top(true)
    .skip_taskbar(true)
    .transparent(true)
    .focused(false)
    .visible(false)
    .build()
    {
        Ok(_) => debug!("Transcription display window created (hidden)"),
        Err(e) => debug!("Failed to create transcription display window: {}", e),
    }
}
```

2. `calculate_transcription_display_position(app_handle: &AppHandle) -> Option<(f64, f64)>` - Position above recording overlay:
```rust
fn calculate_transcription_display_position(app_handle: &AppHandle) -> Option<(f64, f64)> {
    let settings = settings::get_settings(app_handle);

    // If overlay position is None, don't show transcription display either
    if settings.overlay_position == OverlayPosition::None {
        return None;
    }

    let monitor = get_monitor_with_cursor(app_handle)?;
    let work_area = monitor.work_area();
    let scale = monitor.scale_factor();
    let work_area_width = work_area.size.width as f64 / scale;
    let work_area_y = work_area.position.y as f64 / scale;
    let work_area_x = work_area.position.x as f64 / scale;
    let work_area_height = work_area.size.height as f64 / scale;

    // Center horizontally
    let x = work_area_x + (work_area_width - TRANSCRIPTION_DISPLAY_WIDTH) / 2.0;

    // Position based on overlay position setting
    let y = match settings.overlay_position {
        OverlayPosition::Top => {
            // Below recording overlay (overlay is at top)
            work_area_y + OVERLAY_TOP_OFFSET + OVERLAY_HEIGHT + DISPLAY_SPACING
        },
        OverlayPosition::Bottom | OverlayPosition::None => {
            // Above recording overlay (overlay is at bottom)
            work_area_y + work_area_height - OVERLAY_BOTTOM_OFFSET - OVERLAY_HEIGHT - DISPLAY_SPACING - TRANSCRIPTION_DISPLAY_HEIGHT
        },
        OverlayPosition::FollowCursor => {
            // For follow cursor, position above center of screen (static while recording)
            // Don't follow cursor for the display - it would be distracting
            work_area_y + work_area_height - OVERLAY_BOTTOM_OFFSET - OVERLAY_HEIGHT - DISPLAY_SPACING - TRANSCRIPTION_DISPLAY_HEIGHT
        },
    };

    Some((x, y))
}
```

3. `show_transcription_display(app_handle: &AppHandle)`:
```rust
pub fn show_transcription_display(app_handle: &AppHandle) {
    let settings = settings::get_settings(app_handle);
    if settings.overlay_position == OverlayPosition::None {
        return;
    }

    // Ensure window exists
    if app_handle.get_webview_window("transcription_display").is_none() {
        create_transcription_display(app_handle);
    }

    // Update position
    if let Some(window) = app_handle.get_webview_window("transcription_display") {
        if let Some((x, y)) = calculate_transcription_display_position(app_handle) {
            let _ = window.set_position(tauri::Position::Logical(tauri::LogicalPosition { x, y }));
        }
        let _ = window.show();
        let _ = window.emit("show-transcription-display", ());
    }
}
```

4. `hide_transcription_display(app_handle: &AppHandle)`:
```rust
pub fn hide_transcription_display(app_handle: &AppHandle) {
    if let Some(window) = app_handle.get_webview_window("transcription_display") {
        let _ = window.emit("hide-transcription-display", ());
        // Delay hide to allow fade-out animation
        let window_clone = window.clone();
        std::thread::spawn(move || {
            std::thread::sleep(std::time::Duration::from_millis(300));
            let _ = window_clone.hide();
        });
    }
}
```

**In `src-tauri/src/lib.rs`:**
Add `utils::create_transcription_display(app_handle);` right after `utils::create_recording_overlay(app_handle);` in `initialize_core_logic`. Note: check if `utils` re-exports overlay functions or if they're called directly from `overlay`. Follow the existing pattern - the recording overlay is called via `utils::create_recording_overlay` so the transcription display should follow the same routing. If utils.rs wraps overlay.rs functions, add the wrapper there too.
  </action>
  <verify>
Run `cd C:\Users\hsbaz\leadrscribe && cargo check --manifest-path src-tauri/Cargo.toml 2>&1 | head -20` to verify compilation.
  </verify>
  <done>Transcription display window creation, positioning (above recording overlay), show/hide functions exist in overlay.rs. Window is created at app startup. cargo check passes.</done>
</task>

<task type="auto">
  <name>Task 2: Integrate streaming into recording action and wire frontend Channel</name>
  <files>src-tauri/src/actions.rs, src/overlay/TranscriptionDisplay.tsx</files>
  <action>
**Part A: Modify `src-tauri/src/actions.rs` to add streaming during recording**

In `TranscribeAction::start()`, after showing the recording overlay and starting recording, also:
1. Show the transcription display window
2. Start the streaming transcription session by resetting the StreamingSession and setting up the audio-segment listener to feed into StreamingBuffer

The approach: Rather than calling the Tauri command from Rust (which is for frontend->backend), directly use the StreamingSession state and set up the streaming pipeline internally.

Add near the top of the file:
```rust
use crate::commands::streaming::{StreamingSession, StreamingTranscriptionEvent};
use crate::managers::streaming_buffer::StreamingBuffer;
```

Modify `TranscribeAction::start()` - after `STREAMING_STATE.start_recording();` and after `show_recording_overlay(app);`, add:
```rust
// Show transcription display for real-time text
crate::overlay::show_transcription_display(app);

// Initialize streaming session for real-time transcription
let streaming_session = app.state::<Arc<StreamingSession>>();
{
    let mut buffer = streaming_session.buffer.lock().unwrap();
    buffer.reset();
    *streaming_session.is_active.lock().unwrap() = true;
    streaming_session.partial_texts.lock().unwrap().clear();
    streaming_session.pending_chunks.store(0, std::sync::atomic::Ordering::Release);
}
```

Modify the existing segment listener in `setup_segment_listener()` to ALSO feed segments into the streaming buffer. Inside the `app.listen("audio-segment", ...)` callback, after the existing streaming state handling, add:

```rust
// Feed into streaming buffer for real-time display
let streaming_session = app_clone.state::<Arc<StreamingSession>>();
if *streaming_session.is_active.lock().unwrap() {
    let chunk_opt = {
        let mut buffer = streaming_session.buffer.lock().unwrap();
        buffer.add_segment(segment_event.samples.clone())
    };

    if let Some((chunk_samples, chunk_index)) = chunk_opt {
        let pending = streaming_session.pending_chunks.load(std::sync::atomic::Ordering::Acquire);
        if pending < 2 {
            streaming_session.pending_chunks.fetch_add(1, std::sync::atomic::Ordering::AcqRel);
            let tm = tm_clone.clone();
            let session = streaming_session.inner().clone();
            let app_for_emit = app_clone.clone();

            std::thread::spawn(move || {
                match tm.transcribe(chunk_samples) {
                    Ok(text) if !text.is_empty() => {
                        // Store partial text
                        {
                            let mut partials = session.partial_texts.lock().unwrap();
                            if partials.len() <= chunk_index {
                                partials.resize(chunk_index + 1, String::new());
                            }
                            partials[chunk_index] = text.clone();
                        }

                        // Emit to transcription display window
                        if let Some(window) = app_for_emit.get_webview_window("transcription_display") {
                            let _ = window.emit("transcription-partial", serde_json::json!({
                                "text": text,
                                "chunk_index": chunk_index,
                            }));
                        }
                    }
                    Ok(_) => { /* empty transcription, skip */ }
                    Err(e) => {
                        log::error!("Streaming chunk {} transcription failed: {}", chunk_index, e);
                    }
                }
                session.pending_chunks.fetch_sub(1, std::sync::atomic::Ordering::AcqRel);
            });
        } else {
            log::debug!("Dropping streaming chunk {} - concurrency limit reached", chunk_index);
        }
    }
}
```

IMPORTANT NOTE on approach: We're using Tauri events (emit to specific window) rather than Channels here. The Channel API is designed for frontend->backend command invocations, but our streaming is initiated from Rust (shortcut handler). Events targeted to a specific window (window.emit) are acceptable for this moderate throughput (one event every 2-3 seconds). This is NOT the "event spam" the research warned about - that was about emitting every word. We emit one event per chunk (every 2-3 seconds).

In `TranscribeAction::stop()`, after the existing transcription completes, before pasting:
```rust
// Send final result to transcription display, replacing streaming partials
if let Some(window) = ah.get_webview_window("transcription_display") {
    let _ = window.emit("transcription-final", serde_json::json!({
        "text": transcription.clone(),
    }));
}

// Stop streaming session
if let Ok(session) = ah.try_state::<Arc<StreamingSession>>() {
    *session.is_active.lock().unwrap() = false;
}
```

And when hiding the overlay (in the paste section and error paths), also hide the transcription display:
```rust
crate::overlay::hide_transcription_display(&ah_clone);
```

Add `hide_transcription_display` calls everywhere `hide_recording_overlay` is called in the stop() method (there are multiple paths: success, error, empty transcription).

**Part B: Update `src/overlay/TranscriptionDisplay.tsx` to handle the events properly**

The component should already handle `transcription-partial` and `transcription-final` events from Task 1 of Plan 02. Verify the event payload parsing matches what the backend emits:

For `transcription-partial`:
```typescript
const unlistenPartial = await listen<{ text: string; chunk_index: number }>(
  "transcription-partial",
  (event) => {
    const { text } = event.payload;
    // Split new text into words and append to existing words
    const newWords = text.split(/\s+/).filter(w => w.length > 0);
    setWords(prev => [...prev, ...newWords]);
  }
);
```

For `transcription-final`:
```typescript
const unlistenFinal = await listen<{ text: string }>(
  "transcription-final",
  (event) => {
    const { text } = event.payload;
    // Replace all words with final text
    const finalWords = text.split(/\s+/).filter(w => w.length > 0);
    setWords(finalWords);
    setIsListening(false);
    // Keep visible briefly to show final result, then it will be hidden by backend
  }
);
```

For `show-transcription-display`:
```typescript
const unlistenShow = await listen("show-transcription-display", () => {
  setIsVisible(true);
  setIsListening(true);
  setWords([]);  // Clear previous words
});
```

For `hide-transcription-display`:
```typescript
const unlistenHide = await listen("hide-transcription-display", () => {
  setIsVisible(false);
  setIsListening(false);
});
```

Ensure all listeners are cleaned up on unmount.
  </action>
  <verify>
Run `cd C:\Users\hsbaz\leadrscribe && cargo check --manifest-path src-tauri/Cargo.toml 2>&1 | head -20` and `cd C:\Users\hsbaz\leadrscribe && bun run build 2>&1 | tail -10` to verify both backend and frontend compile.
  </verify>
  <done>
- Recording action shows transcription display on start, hides on stop
- Audio segments feed into StreamingBuffer during recording
- Chunks transcribed asynchronously with concurrency limit of 2
- Partial results emitted to transcription display window
- Final batch transcription result replaces streaming partials
- Transcription display hidden after paste/error/empty paths
- Frontend component receives and renders partial/final events
- Both cargo check and bun run build pass
  </done>
</task>

</tasks>

<verification>
1. `cargo check --manifest-path src-tauri/Cargo.toml` passes
2. `bun run build` passes
3. actions.rs shows transcription display in TranscribeAction::start()
4. actions.rs hides transcription display in ALL stop() exit paths
5. Streaming buffer fed from audio-segment listener in setup_segment_listener()
6. Concurrency limited to 2 pending chunks
7. TranscriptionDisplay.tsx handles transcription-partial and transcription-final events
8. overlay.rs has create/show/hide functions for transcription_display window
</verification>

<success_criteria>
- During recording, transcription display window appears above recording overlay
- Partial text streams in every 2-3 seconds as chunks are transcribed
- Words animate in with Framer Motion spring animation
- When recording stops, final batch transcription replaces streaming text
- Display hides after text is pasted
- No GPU OOM from concurrent transcriptions (limited to 2)
- Existing batch transcription flow still works correctly
</success_criteria>

<output>
After completion, create `.planning/phases/08-real-time-transcription-display/08-03-SUMMARY.md`
</output>
